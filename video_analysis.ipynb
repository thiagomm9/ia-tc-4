{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thiag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import mediapipe as mp\n",
    "from moviepy import VideoFileClip, CompositeAudioClip\n",
    "from tqdm import tqdm\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if arm is up\n",
    "def is_arm_up(landmarks, mp_pose):\n",
    "    left_eye = landmarks[mp_pose.PoseLandmark.LEFT_EYE.value]\n",
    "    right_eye = landmarks[mp_pose.PoseLandmark.RIGHT_EYE.value]\n",
    "    left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "    right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "\n",
    "    left_arm_up = left_elbow.y < left_eye.y\n",
    "    right_arm_up = right_elbow.y < right_eye.y\n",
    "\n",
    "    return left_arm_up or right_arm_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect if a person is sitting or standing\n",
    "def detect_posture(landmarks, mp_pose):\n",
    "    try:\n",
    "        # Ensure all necessary landmarks are available\n",
    "        required_landmarks = [\n",
    "            mp_pose.PoseLandmark.LEFT_HIP, \n",
    "            mp_pose.PoseLandmark.RIGHT_HIP, \n",
    "            mp_pose.PoseLandmark.LEFT_KNEE, \n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE, \n",
    "            mp_pose.PoseLandmark.LEFT_ANKLE, \n",
    "            mp_pose.PoseLandmark.RIGHT_ANKLE\n",
    "        ]\n",
    "        \n",
    "        for landmark in required_landmarks:\n",
    "            if landmark not in landmarks or landmarks[landmark] is None:\n",
    "                return False\n",
    "\n",
    "        # Extract the relevant landmarks: hips, knees, and ankles\n",
    "        hip_y = (landmarks[mp_pose.PoseLandmark.LEFT_HIP].y + \n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y) / 2\n",
    "        knee_y = (landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y + \n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].y) / 2\n",
    "        ankle_y = (landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].y + \n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].y) / 2\n",
    "    except (AttributeError, IndexError, KeyError):\n",
    "        # Return False if landmarks are missing or invalid\n",
    "        return False\n",
    "\n",
    "    # Check relative distances\n",
    "    if (hip_y - knee_y) < 0.2 and (knee_y - ankle_y) < 0.2:\n",
    "        return \"Sitting\"\n",
    "    else:\n",
    "        return \"Standing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_poses_faces_and_emotions(video_path, output_path, report_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"There was an error opening the video.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize mediaPipe for pose detection\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose()\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    # Codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec para MP4\n",
    "\n",
    "    # Video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Video writer\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Initialize variables for arm movement detection\n",
    "    arm_up = False\n",
    "    arm_movements_count = 0\n",
    "    \n",
    "    # Initialize variables for emotion detection\n",
    "    emotion = ''\n",
    "\n",
    "    # Initialize frame counter\n",
    "    i=1\n",
    "    # Frame processing loop\n",
    "    for _ in tqdm(range(total_frames), desc=\"Processing\"):    \n",
    "        # Get the current frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Analyse the frame for faces and emotions\n",
    "        face_results = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect poses using MediaPipe\n",
    "        pose_results = pose.process(rgb_frame)\n",
    "        \n",
    "        # Get the face locations and encodings\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "        \n",
    "        # Verify pose landmarks on the frame\n",
    "        if pose_results.pose_landmarks:\n",
    "            # Draw pose landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Verify if the arm is up\n",
    "            if is_arm_up(pose_results.pose_landmarks.landmark, mp_pose):\n",
    "                if not arm_up:\n",
    "                    arm_up = True\n",
    "                    arm_movements_count += 1\n",
    "                    with open(report_path, 'a') as f:\n",
    "                        f.write(f\"on frame {i}: Arm Movement Up Detected\\n\")\n",
    "            else:\n",
    "                arm_up = False\n",
    "\n",
    "            # Show arm movements count on the frame\n",
    "            cv2.putText(frame, f'Arm movements: {arm_movements_count}', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Try to detect if the person is sitting or standing\n",
    "            posture = detect_posture(pose_results.pose_landmarks.landmark, mp_pose)\n",
    "            if posture:\n",
    "                if posture == \"Sitting\":\n",
    "                    with open(report_path, 'a') as f:\n",
    "                        f.write(f\"on frame {i}: Sitting person detected\\n\")\n",
    "                if posture == \"Standing\":\n",
    "                    with open(report_path, 'a') as f:\n",
    "                        f.write(f\"on frame {i}: Standing person detected\\n\")\n",
    "\n",
    "        # Loop through each face detected\n",
    "        for face in face_results:\n",
    "            # Get the bounding box coordinates\n",
    "            x, y, w, h = face['region']['x'], face['region']['y'], face['region']['w'], face['region']['h']\n",
    "            \n",
    "            # Get the emotion\n",
    "            dominant_emotion = face['dominant_emotion']\n",
    "\n",
    "            # Draw around the face\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "            # Write the emotion above the face\n",
    "            cv2.putText(frame, dominant_emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "            \n",
    "            # Write the face emotion to a report file\n",
    "            with open(report_path, 'a') as f:\n",
    "                if emotion != dominant_emotion:\n",
    "                    emotion = dominant_emotion\n",
    "                    f.write(f\"on frame {i}: Face Detected with Dominant Emotion: {emotion}\\n\") \n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Exibir o frame processado\n",
    "        # cv2.imshow('Video', frame)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "        \n",
    "        # if i==200:\n",
    "        #     break\n",
    "        i+=1\n",
    "\n",
    "    # Free resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_audio_to_video(input_video, processed_video, output_video):\n",
    "    # Load original video to extract audio\n",
    "    original_video = VideoFileClip(input_video)\n",
    "    \n",
    "    # Check if the original video has audio\n",
    "    if original_video.audio is None:\n",
    "        print(\"Error: Original video has no audio.\")\n",
    "        return\n",
    "\n",
    "    # Load processed video (no audio)\n",
    "    processed_video_clip = VideoFileClip(processed_video)\n",
    "\n",
    "    new_audioclip = CompositeAudioClip([original_video.audio])\n",
    "\n",
    "    # Combine processed video with original audio\n",
    "    processed_video_clip.audio = new_audioclip\n",
    "\n",
    "    # Write the output video with audio\n",
    "    processed_video_clip.write_videofile(output_video, codec=\"libx264\", audio_codec=\"aac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3326/3326 [1:18:43<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "detect_poses_faces_and_emotions('video.mp4', 'output_video_without_sound.mp4', 'report.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video final_video_with_audio.mp4.\n",
      "MoviePy - Writing audio in final_video_with_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video final_video_with_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready final_video_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "# Add audio back\n",
    "add_audio_to_video(\"video.mp4\", \"output_video_without_sound.mp4\", \"final_video_with_audio.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
